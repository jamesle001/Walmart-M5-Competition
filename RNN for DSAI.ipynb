{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H05UvYiVIVnw"
   },
   "source": [
    "# RNN Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FZqjN4jMIVoK"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVAE7gJIIVoP"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQsN0AwhIVob"
   },
   "source": [
    "## We need to import several things from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ub5oRJWaIVoc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, GRU, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.backend import square, mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ySSddPyKIVoo"
   },
   "source": [
    "## This was developed using Python 3.6 (Anaconda) and package versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 660,
     "status": "ok",
     "timestamp": 1588056568669,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "Y51Qj1IGIVot",
    "outputId": "683369aa-893b-4061-b452-d02a8cfab2af",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1588056570187,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "2EinWDrBIVo4",
    "outputId": "e8e8cb6b-886a-4bd9-9f84-1b1890017c3f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1588056570777,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "d5tPLkVcIVpJ",
    "outputId": "c9fffd19-0ecd-42f2-d4fe-c8260d3bda69"
   },
   "outputs": [],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qUf8dWRyIVpa"
   },
   "source": [
    "## Load the pre-processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1257,
     "status": "ok",
     "timestamp": 1588056578143,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "yUeI5FBHIVpb",
    "outputId": "11aa833b-dcb5-485a-f96c-a021a3338fbb"
   },
   "outputs": [],
   "source": [
    "!ls \"../input/ca-data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCK7H_ulIVpw"
   },
   "outputs": [],
   "source": [
    "path = '../input/ca-data/'\n",
    "ca1_data = pd.read_csv(path+\"CA1_ext.csv\")\n",
    "ca2_data = pd.read_csv(path+\"CA2_ext.csv\")\n",
    "ca3_data = pd.read_csv(path+\"CA_3_ext.csv\")\n",
    "ca4_data = pd.read_csv(path+\"CA4_ext.csv\")\n",
    "tx1_data = pd.read_csv(path+\"TX_1_ext.csv\")\n",
    "tx2_data = pd.read_csv(path+\"TX_2_ext.csv\")\n",
    "tx3_data = pd.read_csv(path+\"TX_3_ext.csv\")\n",
    "wi1_data = pd.read_csv(path+\"WI_1_ext.csv\")\n",
    "wi2_data = pd.read_csv(path+\"WI_2_ext.csv\")\n",
    "wi3_data = pd.read_csv(path+\"WI_3_ext.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1205,
     "status": "ok",
     "timestamp": 1588057879051,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "pjS8hj2ajRS9",
    "outputId": "4eb3af1a-816a-49ec-d2ae-76aead3adfce"
   },
   "outputs": [],
   "source": [
    "data = {}\n",
    "data[\"CA_1\"] = ca1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"CA_2\"] = ca2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"CA_3\"] = ca3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"CA_4\"] = ca4_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"TX_1\"] = tx1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"TX_2\"] = tx2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"TX_3\"] = tx3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"WI_1\"] = wi1_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"WI_2\"] = wi2_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data[\"WI_3\"] = wi3_data[[\"Hobbie_revenue\",\"House_revenue\",\"Foods_revenue\"]]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"CA_1\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of the cities used in the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listofstore = [\"CA_1\",\"CA_2\",\"CA_3\",\"CA_4\",\"TX_1\",\"TX_2\",\"TX_3\",\"WI_1\",\"WI_2\",\"WI_3\"]\n",
    "listofstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tl-iZ5Tyodz1"
   },
   "outputs": [],
   "source": [
    "data_temp = data[\"CA_1\"].join(data[\"CA_2\"], lsuffix='_CA_1', rsuffix='_CA_2')\n",
    "for store in listofstore[2:]:\n",
    "    data_temp1 = data_temp.join(data[store], lsuffix='', rsuffix=store)\n",
    "    data_temp1 = data_temp1.rename(columns={\"Hobbie_revenue\": \"Hobbie_revenue_\"+store,\"House_revenue\": \"House_revenue_\"+store,\"Foods_revenue\": \"Foods_revenue_\"+store})\n",
    "    data_temp = data_temp1\n",
    "    \n",
    "#data_temp = data_temp1.join(data[\"TX_1\"], lsuffix='', rsuffix='_TX_1')\n",
    "\n",
    "data_df = data_temp1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vuZjO5r0IVqp"
   },
   "source": [
    "These are the top rows of the data-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-SENxIsIVqr"
   },
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-op7XuLIVrP"
   },
   "source": [
    "There are 3*10 input-signals in the data-set. There are 1913 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNvQgasiIVrQ"
   },
   "outputs": [],
   "source": [
    "data_df.values.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku_A65D4IVsX"
   },
   "source": [
    "### Add Data\n",
    "\n",
    "We can add some input-signals to the data that may help our model in making predictions.\n",
    "\n",
    "For example, given just a temperature of 10 degrees Celcius the model wouldn't know whether that temperature was measured during the day or the night, or during summer or winter. The model would have to infer this from the surrounding data-points which might not be very accurate for determining whether it's an abnormally warm winter, or an abnormally cold summer, or whether it's day or night. So having this information could make a big difference in how accurately the model can predict the next output.\n",
    "\n",
    "Although the data-set does contain the date and time information for each observation, it is only used in the index so as to order the data. We will therefore add separate input-signals to the data-set for the day-of-year (between 1 and 366) and the hour-of-day (between 0 and 23)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "numdays = 1913\n",
    "base = datetime.datetime(2011, 1, 29)\n",
    "date_list = [base + datetime.timedelta(days=x) for x in range(numdays)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "dayofyearlist = [i.timetuple().tm_yday for i in date_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df[\"Dayofyear\"] = dayofyearlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojJsyuMnIVsj"
   },
   "source": [
    "### Target Data for Prediction\n",
    "\n",
    "We will try and predict the future weather-data for this city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X5IwaoN-IVsp"
   },
   "outputs": [],
   "source": [
    "target_store = 'CA_1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GH2XDXhHIVsx"
   },
   "source": [
    "We will try and predict these signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NhnP686pIVsy"
   },
   "outputs": [],
   "source": [
    "target_names = ['Hobbie_revenue', 'House_revenue', 'Foods_revenue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8_Wo9LFIVs5"
   },
   "source": [
    "The following is the number of time-steps that we will shift the target-data. Our data-set is sampled to have an observation for each day, so there are 30 observations for a month.\n",
    "\n",
    "We want to predict the Revenue 1 month into the future, we shift the data 30 time-steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IaHaEoxvIVs6"
   },
   "outputs": [],
   "source": [
    "shift_months = 1\n",
    "shift_steps = shift_months * 30  # Number of days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UbB7pQSIVtD"
   },
   "outputs": [],
   "source": [
    "data_targets = data[target_store][target_names].shift(-shift_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1930,
     "status": "ok",
     "timestamp": 1588057199714,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "-W6hsBovIVtM",
    "outputId": "6a785551-9993-4297-e3ac-bfcb7627a925"
   },
   "outputs": [],
   "source": [
    "data[target_store][target_names].head(shift_steps + 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1588057228249,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "auH7K3oqIVtc",
    "outputId": "99f0af09-9896-4981-97e9-f35a846189ed",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_targets.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5502,
     "status": "ok",
     "timestamp": 1588057237507,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "fyyQBOVSIVtn",
    "outputId": "16a9866d-afbb-4746-a915-71f1c0b5ccdf"
   },
   "outputs": [],
   "source": [
    "data_targets.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ApByqsvuIVtx"
   },
   "source": [
    "### NumPy Arrays\n",
    "\n",
    "We now convert the Pandas data-frames to NumPy arrays that can be input to the neural network. We also remove the last part of the numpy arrays, because the target-data has `NaN` for the shifted period, and we only want to have valid data and we need the same array-shapes for the input- and output-data.\n",
    "\n",
    "These are the input-signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1208,
     "status": "ok",
     "timestamp": 1588059067202,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "6vzqaXJ4sU3e",
    "outputId": "567c9e88-14be-4b88-c1e7-4bcbc2bf29d1"
   },
   "outputs": [],
   "source": [
    "data_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqFehoGcIVty"
   },
   "outputs": [],
   "source": [
    "x_data = data_df.values[0:-shift_steps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1268,
     "status": "ok",
     "timestamp": 1588059090142,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "3KWUi4byIVuA",
    "outputId": "d1e2dda4-009e-4c59-c8e3-77122fd097ae"
   },
   "outputs": [],
   "source": [
    "print(type(x_data))\n",
    "print(\"Shape:\", x_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bTw9RpMBIVuS"
   },
   "source": [
    "These are the output-signals (or target-signals):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1153,
     "status": "ok",
     "timestamp": 1588059097040,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "9oaccYGcIVuU",
    "outputId": "f7cfdfd9-bf8c-4758-813a-0d97ee973e61"
   },
   "outputs": [],
   "source": [
    "y_data = data_targets.values[:-shift_steps]\n",
    "y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3522,
     "status": "ok",
     "timestamp": 1588059109952,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "BYB0sctGIVuh",
    "outputId": "c9249752-551f-45ae-cf5d-449d30cddcc3",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(type(y_data))\n",
    "print(\"Shape:\", y_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GW2nZn8hIVuq"
   },
   "source": [
    "This is the number of observations (aka. data-points or samples) in the data-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1979,
     "status": "ok",
     "timestamp": 1588059124134,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "RbAjoulTIVur",
    "outputId": "ecca0a6d-5ab5-451c-b6d3-50b98665faab"
   },
   "outputs": [],
   "source": [
    "num_data = len(x_data)\n",
    "num_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCQSYVkOIVuv"
   },
   "source": [
    "This is the fraction of the data-set that will be used for the training-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqnoLkTtIVuw"
   },
   "outputs": [],
   "source": [
    "train_split = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GEgW1i9cIVu8"
   },
   "source": [
    "This is the number of observations in the training-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1294,
     "status": "ok",
     "timestamp": 1588059133821,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "bh-BSwOdIVu9",
    "outputId": "ecb5a8a9-4d77-4a36-c7cd-b8d190a6e330"
   },
   "outputs": [],
   "source": [
    "num_train = int(train_split * num_data)\n",
    "num_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9udFOJcIVvE"
   },
   "source": [
    "This is the number of observations in the test-set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1338,
     "status": "ok",
     "timestamp": 1588059139601,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "Fa2DJvJvIVvF",
    "outputId": "40eb9212-7150-43db-ec3b-2e42109a4f8c"
   },
   "outputs": [],
   "source": [
    "num_test = num_data - num_train\n",
    "num_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPrv0UtfIVvJ"
   },
   "source": [
    "These are the input-signals for the training- and test-sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2551,
     "status": "ok",
     "timestamp": 1588059147951,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "loBBgucjIVvK",
    "outputId": "d8f81e89-4d2a-4c03-c7df-06bb991a6b45"
   },
   "outputs": [],
   "source": [
    "x_train = x_data[0:num_train]\n",
    "x_test = x_data[num_train:]\n",
    "len(x_train) + len(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duLF7f7KIVvR"
   },
   "source": [
    "These are the output-signals for the training- and test-sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1537,
     "status": "ok",
     "timestamp": 1588059154446,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "rZW5vLc-IVvS",
    "outputId": "28958738-e68e-449a-cc2b-8f97f412698f"
   },
   "outputs": [],
   "source": [
    "y_train = y_data[0:num_train]\n",
    "y_test = y_data[num_train:]\n",
    "len(y_train) + len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFgUgX_aIVvZ"
   },
   "source": [
    "This is the number of input-signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1595,
     "status": "ok",
     "timestamp": 1588059161006,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "VvIJlLOHIVva",
    "outputId": "aee18a5f-c960-4b6b-bfc4-cfdcf8b6d1ed"
   },
   "outputs": [],
   "source": [
    "num_x_signals = x_data.shape[1]\n",
    "num_x_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Pj3lVM6IVvd"
   },
   "source": [
    "This is the number of output-signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1892,
     "status": "ok",
     "timestamp": 1588059168572,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "XvKpX9KkIVvd",
    "outputId": "c5cf24d9-296d-4617-f4ea-a317c8b88aa6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_y_signals = y_data.shape[1]\n",
    "num_y_signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57H-vpNlIVvi"
   },
   "source": [
    "### Scaled Data\n",
    "\n",
    "The data-set contains a wide range of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3913,
     "status": "ok",
     "timestamp": 1588059180097,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "9W60yaiDIVvj",
    "outputId": "bedde7e4-15a5-439a-8c0e-973088335c27"
   },
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(x_train))\n",
    "print(\"Max:\", np.max(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PWmy05n1IVvr"
   },
   "source": [
    "The neural network works best on values roughly between -1 and 1, so we need to scale the data before it is being input to the neural network. We can use `scikit-learn` for this.\n",
    "\n",
    "We first create a scaler-object for the input-signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGVl2Xp7IVvs"
   },
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dLQ79tlGIVvw"
   },
   "source": [
    "We then detect the range of values from the training-data and scale the training-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_rgan65YIVvx"
   },
   "outputs": [],
   "source": [
    "x_train_scaled = x_scaler.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YoKABEY5IVv5"
   },
   "source": [
    "Apart from a small rounding-error, the data has been scaled to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1588059199920,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "GdYqh7FCIVv5",
    "outputId": "d1527080-f1e6-4c14-9f09-ce3d54501284"
   },
   "outputs": [],
   "source": [
    "print(\"Min:\", np.min(x_train_scaled))\n",
    "print(\"Max:\", np.max(x_train_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGgj-MT6IVv9"
   },
   "source": [
    "We use the same scaler-object for the input-signals in the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QvlOXz5VIVv9"
   },
   "outputs": [],
   "source": [
    "x_test_scaled = x_scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNkGeOXHIVwC"
   },
   "source": [
    "The target-data comes from the same data-set as the input-signals, because it is the weather-data for one of the cities that is merely time-shifted. But the target-data could be from a different source with different value-ranges, so we create a separate scaler-object for the target-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCDx6DmaIVwD"
   },
   "outputs": [],
   "source": [
    "y_scaler = MinMaxScaler()\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IN_hrZYUIVwJ"
   },
   "source": [
    "## Data Generator\n",
    "\n",
    "The data-set has now been prepared as 2-dimensional numpy arrays. The training-data has almost large observations, consisting of 20 input-signals and 3 output-signals.\n",
    "\n",
    "These are the array-shapes of the input and output data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1339,
     "status": "ok",
     "timestamp": 1588059216379,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "54ef6t9hIVwL",
    "outputId": "13c48e0b-bc47-4d0c-cdf0-766ba60a865b"
   },
   "outputs": [],
   "source": [
    "print(x_train_scaled.shape)\n",
    "print(y_train_scaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfsuPt-zIVwZ"
   },
   "source": [
    "Instead of training the Recurrent Neural Network on the complete sequences of large observations, we will use the following function to create a batch of shorter sub-sequences picked at random from the training-data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0SB40NVIVwa"
   },
   "outputs": [],
   "source": [
    "def batch_generator(batch_size, sequence_length):\n",
    "    \"\"\"\n",
    "    Generator function for creating random batches of training-data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Infinite loop.\n",
    "    while True:\n",
    "        # Allocate a new array for the batch of input-signals.\n",
    "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
    "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
    "\n",
    "        # Allocate a new array for the batch of output-signals.\n",
    "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
    "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
    "\n",
    "        # Fill the batch with random sequences of data.\n",
    "        for i in range(batch_size):\n",
    "            # Get a random start-index.\n",
    "            # This points somewhere into the training-data.\n",
    "            idx = np.random.randint(num_train - sequence_length)\n",
    "            \n",
    "            # Copy the sequences of data starting at this index.\n",
    "            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n",
    "            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n",
    "        \n",
    "        yield (x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4CuxcUvEIVwd"
   },
   "source": [
    "We will use a large batch-size so as to keep the GPU near 100% work-load. You may have to adjust this number depending on your GPU, its RAM and your choice of `sequence_length` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "euU2I0T5IVwe"
   },
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZyoQ4PYIVwi"
   },
   "source": [
    "1. We will use a sequence-length of 1344, which means that each random sequence contains observations for 8 weeks. One time-step corresponds to one hour, so 24 x 7 time-steps corresponds to a week, and 30 x 6 corresponds to 6 months."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1559,
     "status": "ok",
     "timestamp": 1588059255114,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "R4UL3BzHIVwi",
    "outputId": "d8e362e5-77f6-4cf8-ebc6-f78fdbc88d30"
   },
   "outputs": [],
   "source": [
    "sequence_length = 30 * 6\n",
    "sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXkQBlJAIVwm"
   },
   "source": [
    "We then create the batch-generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcTwC6lpIVwn"
   },
   "outputs": [],
   "source": [
    "generator = batch_generator(batch_size=batch_size,\n",
    "                            sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQhjr67FIVwx"
   },
   "source": [
    "We can then test the batch-generator to see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DSXw1qnAIVw8"
   },
   "outputs": [],
   "source": [
    "x_batch, y_batch = next(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSGJl5M_IVxJ"
   },
   "source": [
    "# This gives us a random batch of 256 sequences, each sequence having 180 observations, and each observation having 31 input-signals and 3 output-signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1422,
     "status": "ok",
     "timestamp": 1588059266231,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "cBsE_Q9fIVxQ",
    "outputId": "31074e6a-ba2e-4109-8567-1eab52fb8241"
   },
   "outputs": [],
   "source": [
    "print(x_batch.shape)\n",
    "print(y_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wA3-TexIVxe"
   },
   "source": [
    "We can plot one of the 20 input-signals as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1354,
     "status": "ok",
     "timestamp": 1588059271198,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "HUWslXztIVxe",
    "outputId": "8c4333a5-f4ad-4a7d-85d0-949c0e1a8dc1"
   },
   "outputs": [],
   "source": [
    "batch = 0   # First sequence in the batch.\n",
    "signal = 0  # First signal from the 20 input-signals.\n",
    "seq = x_batch[batch, :, signal]\n",
    "plt.plot(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeS3evLYIVxl"
   },
   "source": [
    "We can also plot one of the output-signals that we want the model to learn how to predict given all those 20 input signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4264,
     "status": "ok",
     "timestamp": 1588059281718,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "V7EbGx_QIVxm",
    "outputId": "4121b978-7d15-4e8e-8279-f370bb02426a"
   },
   "outputs": [],
   "source": [
    "seq = y_batch[batch, :, signal]\n",
    "plt.plot(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xv4rzKKCIVxs"
   },
   "source": [
    "### Validation Set\n",
    "\n",
    "The neural network trains quickly so we can easily run many training epochs. But then there is a risk of overfitting the model to the training-set so it does not generalize well to unseen data. We will therefore monitor the model's performance on the test-set after each epoch and only save the model's weights if the performance is improved on the test-set.\n",
    "\n",
    "The batch-generator randomly selects a batch of short sequences from the training-data and uses that during training. But for the validation-data we will instead run through the entire sequence from the test-set and measure the prediction accuracy on that entire sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3jnbtaJCIVxu"
   },
   "outputs": [],
   "source": [
    "validation_data = (np.expand_dims(x_test_scaled, axis=0),\n",
    "                   np.expand_dims(y_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "srph06VBIVxz"
   },
   "source": [
    "## Create the Recurrent Neural Network\n",
    "\n",
    "We are now ready to create the Recurrent Neural Network (RNN). We will use the Keras API for this because of its simplicity. See Tutorial #03-C for a tutorial on Keras and Tutorial #20 for more information on Recurrent Neural Networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ayEHHlHIVx0"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0EQkoIrIVx9"
   },
   "source": [
    "We can now add a Gated Recurrent Unit (GRU) to the network. This will have 512 outputs for each time-step in the sequence.\n",
    "\n",
    "Note that because this is the first layer in the model, Keras needs to know the shape of its input, which is a batch of sequences of arbitrary length (indicated by `None`), where each observation has a number of input-signals (`num_x_signals`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_wGB8EqNIVx-"
   },
   "outputs": [],
   "source": [
    "model.add(GRU(units=512,\n",
    "              return_sequences=True,\n",
    "              input_shape=(None, num_x_signals,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EL3ALGj0IVyH"
   },
   "source": [
    "The GRU outputs a batch of sequences of 512 values. We want to predict 3 output-signals, so we add a fully-connected (or dense) layer which maps 512 values down to only 3 values.\n",
    "\n",
    "The output-signals in the data-set have been limited to be between 0 and 1 using a scaler-object. So we also limit the output of the neural network using the Sigmoid activation function, which squashes the output to be between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJkkkBokIVyJ"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_y_signals, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46ZZ3BVCIVyR"
   },
   "source": [
    "A problem with using the Sigmoid activation function, is that we can now only output values in the same range as the training-data.\n",
    "\n",
    "We can use a linear activation function on the output instead. This allows for the output to take on arbitrary values. It might work with the standard initialization for a simple network architecture, but for more complicated network architectures e.g. with more layers, it might be necessary to initialize the weights with smaller values to avoid `NaN` values during training. You may need to experiment with this to get it working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-EXds2a_IVyS"
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    from tensorflow.python.keras.initializers import RandomUniform\n",
    "\n",
    "    # Maybe use lower init-ranges.\n",
    "    init = RandomUniform(minval=-0.05, maxval=0.05)\n",
    "\n",
    "    model.add(Dense(num_y_signals,\n",
    "                    activation='linear',\n",
    "                    kernel_initializer=init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acc8Ccl5IVyW"
   },
   "source": [
    "### Loss Function\n",
    "\n",
    "We will use Mean Squared Error (MSE) as the loss-function that will be minimized. This measures how closely the model's output matches the true output signals.\n",
    "\n",
    "However, at the beginning of a sequence, the model has only seen input-signals for a few time-steps, so its generated output may be very inaccurate. Using the loss-value for the early time-steps may cause the model to distort its later output. We therefore give the model a \"warmup-period\" of 30 time-steps where we don't use its accuracy in the loss-function, in hope of improving the accuracy for later time-steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6mZUbI-jIVyX"
   },
   "outputs": [],
   "source": [
    "warmup_steps = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxUwz7DLIVyj"
   },
   "outputs": [],
   "source": [
    "def loss_mse_warmup(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Squared Error between y_true and y_pred,\n",
    "    but ignore the beginning \"warmup\" part of the sequences.\n",
    "    \n",
    "    y_true is the desired output.\n",
    "    y_pred is the model's output.\n",
    "    \"\"\"\n",
    "\n",
    "    # The shape of both input tensors are:\n",
    "    # [batch_size, sequence_length, num_y_signals].\n",
    "\n",
    "    # Ignore the \"warmup\" parts of the sequences\n",
    "    # by taking slices of the tensors.\n",
    "    y_true_slice = y_true[:, warmup_steps:, :]\n",
    "    y_pred_slice = y_pred[:, warmup_steps:, :]\n",
    "\n",
    "    # These sliced tensors both have this shape:\n",
    "    # [batch_size, sequence_length - warmup_steps, num_y_signals]\n",
    "\n",
    "    # Calculat the Mean Squared Error and use it as loss.\n",
    "    mse = mean(square(y_true_slice - y_pred_slice))\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMTG05H_IVyo"
   },
   "source": [
    "### Compile Model\n",
    "\n",
    "This is the optimizer and the beginning learning-rate that we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vks5fX08IVyo"
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sf3by5s8IVyx"
   },
   "source": [
    "We then compile the Keras model so it is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XafVxiyFIVyz"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=loss_mse_warmup, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYuT58usIVy6"
   },
   "source": [
    "This is a very small model with only two layers. The output shape of `(None, None, 3)` means that the model will output a batch with an arbitrary number of sequences, each of which has an arbitrary number of observations, and each observation has 3 signals. This corresponds to the 3 target signals we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1302,
     "status": "ok",
     "timestamp": 1588059335304,
     "user": {
      "displayName": "Joe V Akkara",
      "photoUrl": "",
      "userId": "12460244073478755788"
     },
     "user_tz": -480
    },
    "id": "f10Iy3zGIVy7",
    "outputId": "6fbb614a-e48a-4dd6-b2c5-2e8a90452dc9"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe2Len81IVzC"
   },
   "source": [
    "### Callback Functions\n",
    "\n",
    "During training we want to save checkpoints and log the progress to TensorBoard so we create the appropriate callbacks for Keras.\n",
    "\n",
    "This is the callback for writing checkpoints during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sHONCRLWIVzD"
   },
   "outputs": [],
   "source": [
    "path_checkpoint = '23_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1NcERvTIVzW"
   },
   "source": [
    "This is the callback for stopping the optimization when performance worsens on the validation-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3NdU13sIVzW"
   },
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lR7iw6KzIVzZ"
   },
   "source": [
    "This is the callback for writing the TensorBoard log during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4erTIKtIVza"
   },
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./23_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17R0UUSRIVzc"
   },
   "source": [
    "This callback reduces the learning-rate for the optimizer if the validation-loss has not improved since the last epoch (as indicated by `patience=0`). The learning-rate will be reduced by multiplying it with the given factor. We set a start learning-rate of 1e-3 above, so multiplying it by 0.1 gives a learning-rate of 1e-4. We don't want the learning-rate to go any lower than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RIxtDwVMIVzd"
   },
   "outputs": [],
   "source": [
    "callback_reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                                       factor=0.1,\n",
    "                                       min_lr=1e-4,\n",
    "                                       patience=0,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWewcbqqIVzm"
   },
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard,\n",
    "             callback_reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9jLbnMB3IVzw"
   },
   "source": [
    "## Train the Recurrent Neural Network\n",
    "\n",
    "We can now train the neural network.\n",
    "\n",
    "Note that a single \"epoch\" does not correspond to a single processing of the training-set, because of how the batch-generator randomly selects sub-sequences from the training-set. Instead we have selected `steps_per_epoch` so that one \"epoch\" is processed in a few minutes.\n",
    "\n",
    "Also note that the loss sometimes becomes `NaN` (not-a-number). This is often resolved by restarting and running the Notebook again. But it may also be caused by your neural network architecture, learning-rate, batch-size, sequence-length, etc. in which case you may have to modify those settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1s6Yu4RIVzw",
    "outputId": "87b9d82a-cbdc-40bf-8b5b-5aaee0ed8eb4"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.fit(x=generator,\n",
    "          epochs=20,\n",
    "          steps_per_epoch=100,\n",
    "          validation_data=validation_data,\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiYnAkzjIVz3"
   },
   "source": [
    "### Load Checkpoint\n",
    "\n",
    "Because we use early-stopping when training the model, it is possible that the model's performance has worsened on the test-set for several epochs before training was stopped. We therefore reload the last saved checkpoint, which should have the best performance on the test-set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfdj8U5XIVz5"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHiFAwHcIVz9"
   },
   "source": [
    "## Performance on Test-Set\n",
    "\n",
    "We can now evaluate the model's performance on the test-set. This function expects a batch of data, but we will just use one long time-series for the test-set, so we just expand the array-dimensionality to create a batch with that one sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SnY5WhZ8IV0A"
   },
   "outputs": [],
   "source": [
    "result = model.evaluate(x=np.expand_dims(x_test_scaled, axis=0),\n",
    "                        y=np.expand_dims(y_test_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UccTM0ZDIV0H"
   },
   "outputs": [],
   "source": [
    "print(\"loss (test-set):\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rDBqn6NTIV0L"
   },
   "outputs": [],
   "source": [
    "# If you have several metrics you can use this instead.\n",
    "if False:\n",
    "    for res, metric in zip(result, model.metrics_names):\n",
    "        print(\"{0}: {1:.3e}\".format(metric, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzgXCsFiIV0N"
   },
   "source": [
    "## Generate Predictions\n",
    "\n",
    "This helper-function plots the predicted and true output-signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rFTPs67zIV0O"
   },
   "outputs": [],
   "source": [
    "def plot_comparison(start_idx, length=100, train=True):\n",
    "    \"\"\"\n",
    "    Plot the predicted and true output-signals.\n",
    "    \n",
    "    :param start_idx: Start-index for the time-series.\n",
    "    :param length: Sequence-length to process and plot.\n",
    "    :param train: Boolean whether to use training- or test-set.\n",
    "    \"\"\"\n",
    "    \n",
    "    if train:\n",
    "        # Use training-data.\n",
    "        x = x_train_scaled\n",
    "        y_true = y_train\n",
    "    else:\n",
    "        # Use test-data.\n",
    "        x = x_test_scaled\n",
    "        y_true = y_test\n",
    "    \n",
    "    # End-index for the sequences.\n",
    "    end_idx = start_idx + length\n",
    "    \n",
    "    # Select the sequences from the given start-index and\n",
    "    # of the given length.\n",
    "    x = x[start_idx:end_idx]\n",
    "    y_true = y_true[start_idx:end_idx]\n",
    "    \n",
    "    # Input-signals for the model.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Use the model to predict the output-signals.\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    # The output of the model is between 0 and 1.\n",
    "    # Do an inverse map to get it back to the scale\n",
    "    # of the original data-set.\n",
    "    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n",
    "    \n",
    "    # For each output-signal.\n",
    "    for signal in range(len(target_names)):\n",
    "        # Get the output-signal predicted by the model.\n",
    "        signal_pred = y_pred_rescaled[:, signal]\n",
    "        \n",
    "        # Get the true output-signal from the data-set.\n",
    "        signal_true = y_true[:, signal]\n",
    "\n",
    "        # Make the plotting-canvas bigger.\n",
    "        plt.figure(figsize=(15,5))\n",
    "        \n",
    "        # Plot and compare the two signals.\n",
    "        plt.plot(signal_true, label='true')\n",
    "        plt.plot(signal_pred, label='pred')\n",
    "        \n",
    "        # Plot grey box for warmup-period.\n",
    "        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n",
    "        \n",
    "        # Plot labels etc.\n",
    "        plt.ylabel(target_names[signal])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD37qudaIV0Y"
   },
   "source": [
    "We can now plot an example of predicted output-signals. \n",
    "\n",
    "The prediction is not very accurate for the first 30-50 time-steps because the model has seen very little input-data at this point.\n",
    "The model generates a single time-step of output data for each time-step of the input-data, so when the model has only run for a few time-steps, it knows very little of the history of the input-signals and cannot make an accurate prediction. The model needs to \"warm up\" by processing perhaps 30-50 time-steps before its predicted output-signals can be used.\n",
    "\n",
    "That is why we ignore this \"warmup-period\" of 50 time-steps when calculating the mean-squared-error in the loss-function. The \"warmup-period\" is shown as a grey box in these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2dNkPv1IV0b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_comparison(start_idx=0, length=1600, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kj_hT8tBIV0f"
   },
   "source": [
    "The model was able to predict the overall oscillations quite well but the peaks were sometimes inaccurate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVtxlYkVIV0p"
   },
   "outputs": [],
   "source": [
    "data[\"CA_4\"]['Hobbie_revenue'][1000:1000+500].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2dZrpA8IV0t"
   },
   "source": [
    "### Example from Test-Set\n",
    "\n",
    "Now consider an example from the test-set. The model has not seen this data during training.\n",
    "\n",
    "The temperature is predicted reasonably well, although the peaks are sometimes inaccurate.\n",
    "\n",
    "The wind-speed has not been predicted so well. The daily oscillation-frequency seems to match, but the center-level and the peaks are quite inaccurate. A guess would be that the wind-speed is difficult to predict from the given input data, so the model has merely learnt to output sinusoidal oscillations in the daily frequency and approximately at the right center-level.\n",
    "\n",
    "The atmospheric pressure is predicted reasonably well, except for a lag and a more noisy signal than the true time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rhLobZHPIV0u",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_comparison(start_idx=10, length=100, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionofrevenue():\n",
    "    \"\"\"\n",
    "    Predict values\n",
    "    \"\"\"\n",
    "    x_full = data_df.values\n",
    "    x_full_scaled = x_scaler.transform(x_full)\n",
    "    \n",
    "    # Select the sequences from the given start-index and\n",
    "    # of the given length.\n",
    "    x = x_full\n",
    "    print(y_data.shape)\n",
    "    y_true = np.append(y_data,np.zeros((30,3)),axis = 0)\n",
    "    print(y_true.shape)\n",
    "    \n",
    "    # Input-signals for the model.\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # Use the model to predict the output-signals.\n",
    "    y_pred = model.predict(x)\n",
    "    \n",
    "    # The output of the model is between 0 and 1.\n",
    "    # Do an inverse map to get it back to the scale\n",
    "    # of the original data-set.\n",
    "    y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n",
    "    \n",
    "    # For each output-signal.\n",
    "    for signal in range(len(target_names)):\n",
    "        # Get the output-signal predicted by the model.\n",
    "        signal_pred = y_pred_rescaled[:, signal]\n",
    "        \n",
    "        # Get the true output-signal from the data-set.\n",
    "        signal_true = y_true[:, signal]\n",
    "\n",
    "        # Make the plotting-canvas bigger.\n",
    "        plt.figure(figsize=(15,5))\n",
    "        \n",
    "        # Plot and compare the two signals.\n",
    "        plt.plot(signal_true, label='true')\n",
    "        plt.plot(signal_pred, label='pred')\n",
    "        \n",
    "        # Plot grey box for warmup-period.\n",
    "        p = plt.axvspan(0, warmup_steps, facecolor='black', alpha=0.15)\n",
    "        \n",
    "        # Plot labels etc.\n",
    "        plt.ylabel(target_names[signal])\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionofrevenue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCLRenZqIV0y"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "Used a Recurrent Neural Network to predict several time-series from a number of input-signals. We used revenue-data for 10 stores to predict next months revenue for one of the stores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9U-KxseIV0y"
   },
   "source": [
    "## Exercises\n",
    "\n",
    "These are a few suggestions for exercises that may help improve your skills with TensorFlow. It is important to get hands-on experience with TensorFlow in order to learn how to use it properly.\n",
    "\n",
    "You may want to backup this Notebook before making any changes.\n",
    "\n",
    "* Remove the wind-speed from the target-data. Does it improve prediction for the temperature and pressure?\n",
    "* Train for more epochs, possibly with a lower learning-rate. Does it improve the performance on the test-set?\n",
    "* Try a different architecture for the neural network, e.g. higher or lower state-size for the GRU layer, more GRU layers, dense layers before and after the GRU layers, etc.\n",
    "* Use hyper-parameter optimization from Tutorial #19.\n",
    "* Try using longer and shorter sequences for the batch-generator.\n",
    "* Try and remove the city \"Odense\" from the input-signals.\n",
    "* Try and add last year's weather-data to the input-signals.\n",
    "* How good is the model at predicting the weather 3 or 7 days into the future?\n",
    "* Can you train a single model with the output-signals for multiple time-shifts, so that a single model predicts the weather in e.g. 1, 3 and 7 days.\n",
    "* Explain to a friend how the program works."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "name": "23_Time-Series-Prediction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
